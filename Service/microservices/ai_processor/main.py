import os
import asyncio
import logging
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any

from dotenv import load_dotenv
from aiokafka import AIOKafkaConsumer, AIOKafkaProducer
from aiokafka.errors import KafkaConnectionError, GroupCoordinatorNotAvailableError, KafkaError
from aiokafka.admin import AIOKafkaAdminClient, NewTopic

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π —Å–µ—Ä–≤–∏—Å–∞
from pipeline import AtelectasisPipeline
from dicom_handler import DicomHandler
from detector import AtelectasisDetector

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP", "kafka:9092")
TOPIC_RAW = os.getenv("TOPIC_RAW", "raw-images")
TOPIC_PROC = os.getenv("TOPIC_PROC", "inference-results")
GROUP_ID = "ai-processor-group"
MODEL_PATH = os.getenv("MODEL_PATH", "./model/best_deit_scm_model.pth")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "./output")

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.join(OUTPUT_DIR, "json_reports"), exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
pipeline: AtelectasisPipeline = None
detector: AtelectasisDetector = None


async def wait_for_kafka_ready(bootstrap_servers, max_retries=15, delay=5):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka"""
    for attempt in range(max_retries):
        try:
            producer = AIOKafkaProducer(
                bootstrap_servers=bootstrap_servers,
                request_timeout_ms=5000,
                connections_max_idle_ms=10000
            )
            await producer.start()
            logger.info(f"‚úÖ Kafka is ready! Connection successful")
            await producer.stop()
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kafka not ready (attempt {attempt + 1}/{max_retries}): {type(e).__name__}: {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Kafka not ready after maximum retries")


async def start_with_retries(component, max_retries=10, delay=3):
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    for attempt in range(max_retries):
        try:
            await component.start()
            logger.info(f"‚úÖ Component started successfully")
            return
        except (KafkaConnectionError, GroupCoordinatorNotAvailableError, KafkaError) as e:
            logger.warning(f"‚ö†Ô∏è Failed to start component (attempt {attempt + 1}/{max_retries}): {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Cannot start component after retries")


async def ensure_topics(bootstrap, topics):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–ø–∏–∫–æ–≤ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
    admin = AIOKafkaAdminClient(bootstrap_servers=bootstrap)
    await start_with_retries(admin, max_retries=5)

    try:
        existing = await admin.list_topics()
        to_create = [
            NewTopic(name=topic, num_partitions=1, replication_factor=1)
            for topic in topics if topic not in existing
        ]

        if to_create:
            await admin.create_topics(to_create)
            logger.info(f"‚úÖ Topics created: {[t.name for t in to_create]}")
        else:
            logger.info("‚úÖ All topics already exist")
    finally:
        await admin.close()


def initialize_ai_components():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ò–ò"""
    global pipeline, detector

    logger.info("ü§ñ Initializing AI components...")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
    if not os.path.exists(MODEL_PATH):
        logger.error(f"‚ùå Model file not found at {MODEL_PATH}")
        raise FileNotFoundError(f"Model file not found at {MODEL_PATH}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º pipeline
    pipeline = AtelectasisPipeline(MODEL_PATH, OUTPUT_DIR)
    detector = AtelectasisDetector(MODEL_PATH)

    logger.info("‚úÖ AI components initialized successfully")


async def process_dicom_message(message_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ DICOM —Ñ–∞–π–ª–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è Kafka
    """
    study_id = message_data.get("study_id")
    file_path = message_data.get("file_path")
    timestamp = message_data.get("timestamp")

    logger.info(f"üîç Processing DICOM file: {file_path}")

    start_time = time.time()

    try:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª —á–µ—Ä–µ–∑ pipeline
        result = pipeline.process_dicom(file_path)

        if result["status"] == "success":
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            processing_time = time.time() - start_time

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è Kafka
            kafka_result = {
                "study_id": study_id,
                "status": "completed",
                "processing_time": processing_time,
                "timestamp_received": timestamp,
                "timestamp_processed": datetime.now().isoformat(),
                "results": result["results"],
                "json_report_path": result.get("json_report"),
                "original_dicom_path": file_path,
                "error": None
            }

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ ‚â§5 —Å–µ–∫)
            if processing_time > 5.0:
                logger.warning(f"‚ö†Ô∏è Processing time exceeded 5 seconds: {processing_time:.2f}s")
            else:
                logger.info(f"‚úÖ Processing completed in {processing_time:.2f}s")

            return kafka_result

        else:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
            return {
                "study_id": study_id,
                "status": "error",
                "processing_time": time.time() - start_time,
                "timestamp_received": timestamp,
                "timestamp_processed": datetime.now().isoformat(),
                "results": None,
                "original_dicom_path": file_path,
                "error": result.get("error", "Unknown error during processing")
            }

    except Exception as e:
        logger.error(f"‚ùå Error processing DICOM: {str(e)}")
        return {
            "study_id": study_id,
            "status": "error",
            "processing_time": time.time() - start_time,
            "timestamp_received": timestamp,
            "timestamp_processed": datetime.now().isoformat(),
            "results": None,
            "error": str(e)
        }


async def process_loop():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    logger.info("üöÄ Starting AI Processing Service...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ò–ò
    initialize_ai_components()

    # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka
    await wait_for_kafka_ready(KAFKA_BOOTSTRAP)

    # –°–æ–∑–¥–∞–µ–º —Ç–æ–ø–∏–∫–∏
    await ensure_topics(KAFKA_BOOTSTRAP, [TOPIC_RAW, TOPIC_PROC])

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
    logger.info("‚è≥ Waiting for system stabilization...")
    await asyncio.sleep(10)

    # –°–æ–∑–¥–∞–µ–º consumer –∏ producer
    consumer = AIOKafkaConsumer(
        TOPIC_RAW,
        bootstrap_servers=KAFKA_BOOTSTRAP,
        group_id=GROUP_ID,
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        auto_commit_interval_ms=1000,
        consumer_timeout_ms=1000,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    producer = AIOKafkaProducer(
        bootstrap_servers=KAFKA_BOOTSTRAP,
        value_serializer=lambda v: json.dumps(v).encode('utf-8'),
        acks='all'
    )

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
    await start_with_retries(consumer)
    await start_with_retries(producer)

    logger.info("‚úÖ AI Processing Service ready, waiting for messages...")

    try:
        while True:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                msg_batch = await consumer.getmany(timeout_ms=1000, max_records=10)

                if not msg_batch:
                    continue

                for topic_partition, messages in msg_batch.items():
                    for msg in messages:
                        logger.info(f"üì® Received message from {topic_partition}, offset: {msg.offset}")

                        try:
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
                            message_data = msg.value

                            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è
                            if not isinstance(message_data, dict) or "file_path" not in message_data:
                                logger.error(f"‚ùå Invalid message format: {message_data}")
                                continue

                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º DICOM
                            result = await process_dicom_message(message_data)

                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                            await producer.send_and_wait(TOPIC_PROC, result)
                            logger.info(f"‚úÖ Result sent to {TOPIC_PROC} for study_id: {result['study_id']}")

                        except Exception as e:
                            logger.error(f"‚ùå Error processing message: {e}")
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
                            error_result = {
                                "study_id": message_data.get("study_id", "unknown"),
                                "status": "error",
                                "error": str(e),
                                "timestamp_processed": datetime.now().isoformat()
                            }
                            await producer.send_and_wait(TOPIC_PROC, error_result)

            except GroupCoordinatorNotAvailableError as e:
                logger.warning(f"‚ö†Ô∏è GroupCoordinator not available: {e}, retrying...")
                await asyncio.sleep(5)
                continue
            except Exception as e:
                logger.error(f"‚ùå Error in processing loop: {e}")
                await asyncio.sleep(5)
                continue

    except KeyboardInterrupt:
        logger.info("üõë Shutting down AI Processing Service...")
    finally:
        await consumer.stop()
        await producer.stop()
        logger.info("‚úÖ AI Processing Service stopped")


if __name__ == "__main__":
    try:
        asyncio.run(process_loop())
    except KeyboardInterrupt:
        logger.info("üëã AI Processing Service terminated by user")