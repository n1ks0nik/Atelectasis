import os, sys
import asyncio
import logging
import json
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Optional
import uuid

import uvicorn
from fastapi import FastAPI, UploadFile, HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import FileResponse
from pydantic import BaseModel
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaConnectionError
from dotenv import load_dotenv
import pydicom

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π —Å–µ—Ä–≤–∏—Å–∞
from validators import DicomValidator
from dicom_utils import create_dicom_from_png

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)
from  shared.database import db_manager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP", "kafka:9092")
TOPIC_RAW = os.getenv("TOPIC_RAW", "raw-images")
TOPIC_RESULTS = os.getenv("TOPIC_PROC", "inference-results")
UPLOAD_DIR = "./uploads"
MAX_FILE_SIZE = int(os.getenv("MAX_FILE_SIZE", "104857600"))  # 100MB

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–æ–∫
os.makedirs(UPLOAD_DIR, exist_ok=True)

app = FastAPI(title="Atelectasis Detection API", version="1.0.0")
security = HTTPBearer()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π producer –¥–ª—è Kafka
producer: Optional[AIOKafkaProducer] = None


# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ProcessingRequest(BaseModel):
    study_id: str
    file_path: str
    timestamp: str
    metadata: dict


class ProcessingResponse(BaseModel):
    study_id: str
    status: str
    message: str


class HealthResponse(BaseModel):
    status: str
    kafka_connected: bool
    timestamp: str


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Kafka
async def wait_for_kafka_ready(bootstrap_servers, max_retries=15, delay=5):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka"""
    for attempt in range(max_retries):
        try:
            test_producer = AIOKafkaProducer(
                bootstrap_servers=bootstrap_servers,
                request_timeout_ms=5000,
                connections_max_idle_ms=10000
            )
            await test_producer.start()
            logger.info("‚úÖ Kafka is ready!")
            await test_producer.stop()
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kafka not ready (attempt {attempt + 1}/{max_retries}): {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Kafka not ready after maximum retries")


# –°–æ–±—ã—Ç–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    global producer

    logger.info("üöÄ Starting API Gateway Service...")

    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
    try:
        await db_manager.connect()
        logger.info("‚úÖ Connected to PostgreSQL")
    except Exception as e:
        logger.error(f"‚ùå Failed to connect to database: {e}")
        raise

    # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka
    try:
        await wait_for_kafka_ready(KAFKA_BOOTSTRAP)

        # –°–æ–∑–¥–∞–µ–º producer
        producer = AIOKafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',
            retry_backoff_ms=2000,
            request_timeout_ms=30000
        )
        await producer.start()
        logger.info("‚úÖ Kafka producer started successfully")

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Kafka: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ"""
    global producer

    # –û—Ç–∫–ª—é—á–∞–µ–º—Å—è –æ—Ç –ë–î
    await db_manager.disconnect()
    logger.info("‚úÖ Disconnected from PostgreSQL")

    if producer:
        await producer.stop()
        logger.info("‚úÖ Kafka producer stopped")


# –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ JWT —Ç–æ–∫–µ–Ω–∞ (–∑–∞–≥–ª—É—à–∫–∞)
async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ JWT —Ç–æ–∫–µ–Ω–∞"""
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É JWT
    token = credentials.credentials
    if not token:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials"
        )
    return token


# Endpoints
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    kafka_connected = producer is not None and not producer._closed

    return HealthResponse(
        status="healthy" if kafka_connected else "degraded",
        kafka_connected=kafka_connected,
        timestamp=datetime.now().isoformat()
    )


@app.post("/analyze", response_model=ProcessingResponse)
async def analyze_dicom(
        file: UploadFile,
        credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ DICOM —Ñ–∞–π–ª–∞
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    if file.size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail={"error_code": 413, "message": "–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π", "severity": "critical"}
        )

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    study_id = str(uuid.uuid4())
    temp_file_path = None

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤—Ä–µ–º–µ–Ω–Ω–æ
        temp_file_path = os.path.join(UPLOAD_DIR, f"{study_id}_{file.filename}")

        with open(temp_file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º DICOM
        is_valid, error = DicomValidator.validate_dicom_file(temp_file_path)
        if not is_valid:
            raise HTTPException(
                status_code=error["error_code"],
                detail=error
            )

        # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è Kafka
        kafka_message = {
            "study_id": study_id,
            "file_path": temp_file_path,
            "filename": file.filename,
            "timestamp": datetime.now().isoformat(),
            "user_token": credentials.credentials,
            "metadata": {
                "content_type": file.content_type,
                "file_size": file.size
            }
        }

        await db_manager.create_study(
            study_id=study_id,
            study_instance_uid=pydicom.dcmread(temp_file_path).StudyInstanceUID,
            user_token=credentials.credentials,
            filename=file.filename,
            file_size=file.size
        )

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka
        await producer.send_and_wait(TOPIC_RAW, kafka_message)

        logger.info(f"‚úÖ File {file.filename} sent to processing, study_id: {study_id}")

        return ProcessingResponse(
            study_id=study_id,
            status="processing",
            message="–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É"
        )

    except HTTPException:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        raise

    except KafkaConnectionError as e:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ Kafka
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logger.error(f"‚ùå Kafka error: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail={"error_code": 503, "message": "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω", "severity": "critical"}
        )

    except Exception as e:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π –æ—à–∏–±–∫–µ
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logger.error(f"‚ùå Unexpected error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error_code": 500, "message": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞", "severity": "critical"}
        )


@app.get("/result/{study_id}")
async def get_result(
        study_id: str,
        credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ study_id
    """
    # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –ë–î
    study = await db_manager.get_study(study_id)

    if not study:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Study not found"
        )

    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    response = {
        "study_id": study_id,
        "status": study['status'],
        "created_at": study['created_at'].isoformat() if study['created_at'] else None,
        "updated_at": study['updated_at'].isoformat() if study['updated_at'] else None
    }

    if study['status'] == 'completed':
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        if study['result_status']:
            response['results'] = {
                "status": study['result_status'],
                "atelectasis_probability": float(study['atelectasis_probability']) if study[
                    'atelectasis_probability'] else None,
                "processing_time": float(study['processing_time']) if study['processing_time'] else None,
                "conclusion": study['conclusion'],
                "location": study['location_description']
            }

            if study['bbox_xmin'] is not None:
                response['results']['bbox'] = [
                    study['bbox_xmin'],
                    study['bbox_ymin'],
                    study['bbox_xmax'],
                    study['bbox_ymax']
                ]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
        report_paths = await db_manager.get_report_paths(study_id)
        if report_paths:
            response['reports'] = report_paths

    elif study['status'] == 'error':
        response['error'] = study['error_message']

    return response


@app.get("/statistics")
async def get_statistics(
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã
    """
    stats = await db_manager.get_statistics()
    return stats


@app.post("/test/create_dicom")
async def create_test_dicom(
        png_file: UploadFile,
        add_patient_info: bool = True
):
    """
    –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DICOM –∏–∑ PNG (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
    """
    if not png_file.filename.endswith('.png'):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="–¢—Ä–µ–±—É–µ—Ç—Å—è PNG —Ñ–∞–π–ª"
        )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PNG –≤—Ä–µ–º–µ–Ω–Ω–æ
    temp_png = os.path.join(UPLOAD_DIR, f"temp_{png_file.filename}")
    temp_dcm = os.path.join(UPLOAD_DIR, f"test_{png_file.filename.replace('.png', '.dcm')}")

    try:
        with open(temp_png, "wb") as buffer:
            content = await png_file.read()
            buffer.write(content)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DICOM
        create_dicom_from_png(temp_png, temp_dcm, add_patient_info)

        return {
            "status": "success",
            "dicom_path": temp_dcm,
            "message": "DICOM —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω"
        }

    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π PNG
        if os.path.exists(temp_png):
            os.remove(temp_png)


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )