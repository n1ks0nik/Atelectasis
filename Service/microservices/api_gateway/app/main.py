import os, sys
import asyncio
import logging
import json
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Optional
import uuid
import zipfile
import io

import uvicorn
from fastapi import FastAPI, UploadFile, HTTPException, Depends, status, Request, Form
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import FileResponse, HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaConnectionError
from dotenv import load_dotenv
import pydicom

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π —Å–µ—Ä–≤–∏—Å–∞
from validators import DicomValidator
from dicom_utils import create_dicom_from_png

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(project_root)
from shared.database import db_manager

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP", "kafka:9092")
TOPIC_RAW = os.getenv("TOPIC_RAW", "raw-images")
TOPIC_RESULTS = os.getenv("TOPIC_PROC", "inference-results")
UPLOAD_DIR = "./uploads"
MAX_FILE_SIZE = int(os.getenv("MAX_FILE_SIZE", "104857600"))  # 100MB

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs("static", exist_ok=True)
os.makedirs("templates", exist_ok=True)

app = FastAPI(title="Atelectasis Detection API", version="1.0.0")
security = HTTPBearer()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –∏ —à–∞–±–ª–æ–Ω–æ–≤
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π producer –¥–ª—è Kafka
producer: Optional[AIOKafkaProducer] = None

# –ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
VALID_TOKENS = {
    "demo_token_123": {"username": "demo_user", "role": "user"},
    "admin_token_456": {"username": "admin", "role": "admin"},
    "test_token_789": {"username": "test_user", "role": "user"}
}


# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ProcessingRequest(BaseModel):
    study_id: str
    file_path: str
    timestamp: str
    metadata: dict


class ProcessingResponse(BaseModel):
    study_id: str
    status: str
    message: str


class HealthResponse(BaseModel):
    status: str
    kafka_connected: bool
    timestamp: str


class LoginRequest(BaseModel):
    token: str


# –§—É–Ω–∫—Ü–∏–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
def verify_token_simple(token: str):
    """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞"""
    return VALID_TOKENS.get(token)


async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ JWT —Ç–æ–∫–µ–Ω–∞"""
    token = credentials.credentials
    user_data = verify_token_simple(token)
    if not user_data:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication token"
        )
    return {"token": token, **user_data}


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Kafka
async def wait_for_kafka_ready(bootstrap_servers, max_retries=15, delay=5):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka"""
    for attempt in range(max_retries):
        try:
            test_producer = AIOKafkaProducer(
                bootstrap_servers=bootstrap_servers,
                request_timeout_ms=5000,
                connections_max_idle_ms=10000
            )
            await test_producer.start()
            logger.info("‚úÖ Kafka is ready!")
            await test_producer.stop()
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kafka not ready (attempt {attempt + 1}/{max_retries}): {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Kafka not ready after maximum retries")


# –°–æ–±—ã—Ç–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    global producer

    logger.info("üöÄ Starting API Gateway Service...")

    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –ë–î
    try:
        await db_manager.connect()
        logger.info("‚úÖ Connected to PostgreSQL")
    except Exception as e:
        logger.error(f"‚ùå Failed to connect to database: {e}")
        raise

    # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka
    try:
        await wait_for_kafka_ready(KAFKA_BOOTSTRAP)

        # –°–æ–∑–¥–∞–µ–º producer
        producer = AIOKafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',
            retry_backoff_ms=2000,
            request_timeout_ms=30000
        )
        await producer.start()
        logger.info("‚úÖ Kafka producer started successfully")

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Kafka: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ"""
    global producer

    # –û—Ç–∫–ª—é—á–∞–µ–º—Å—è –æ—Ç –ë–î
    await db_manager.disconnect()
    logger.info("‚úÖ Disconnected from PostgreSQL")

    if producer:
        await producer.stop()
        logger.info("‚úÖ Kafka producer stopped")


# ========== WEB INTERFACE ROUTES ==========

@app.get("/", response_class=HTMLResponse)
async def dashboard(request: Request):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –¥–∞—à–±–æ—Ä–¥"""
    return templates.TemplateResponse("dashboard.html", {"request": request})


@app.get("/upload", response_class=HTMLResponse)
async def upload_page(request: Request):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤"""
    return templates.TemplateResponse("upload.html", {"request": request})


@app.get("/results", response_class=HTMLResponse)
async def results_page(request: Request):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    return templates.TemplateResponse("results.html", {"request": request})


@app.get("/login", response_class=HTMLResponse)
async def login_page(request: Request):
    """–°—Ç—Ä–∞–Ω–∏—Ü–∞ –≤—Ö–æ–¥–∞"""
    return templates.TemplateResponse("login.html", {"request": request})


# ========== API ROUTES ==========

@app.post("/api/login")
async def login(login_data: LoginRequest):
    """API –¥–ª—è –≤—Ö–æ–¥–∞ –≤ —Å–∏—Å—Ç–µ–º—É"""
    user_data = verify_token_simple(login_data.token)
    if not user_data:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token"
        )

    return {
        "status": "success",
        "user": user_data,
        "token": login_data.token
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    kafka_connected = producer is not None and not producer._closed

    return HealthResponse(
        status="healthy" if kafka_connected else "degraded",
        kafka_connected=kafka_connected,
        timestamp=datetime.now().isoformat()
    )


@app.post("/analyze", response_model=ProcessingResponse)
async def analyze_dicom(
        file: UploadFile,
        user_data: dict = Depends(verify_token)
):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ DICOM —Ñ–∞–π–ª–∞
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    if file.size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail={"error_code": 413, "message": "–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π", "severity": "critical"}
        )

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    study_id = str(uuid.uuid4())
    temp_file_path = None

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤—Ä–µ–º–µ–Ω–Ω–æ
        temp_file_path = os.path.join(UPLOAD_DIR, f"{study_id}_{file.filename}")

        with open(temp_file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º DICOM
        is_valid, error = DicomValidator.validate_dicom_file(temp_file_path)
        if not is_valid:
            raise HTTPException(
                status_code=error["error_code"],
                detail=error
            )

        # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è Kafka
        kafka_message = {
            "study_id": study_id,
            "file_path": temp_file_path,
            "filename": file.filename,
            "timestamp": datetime.now().isoformat(),
            "user_token": user_data["token"],
            "metadata": {
                "content_type": file.content_type,
                "file_size": file.size
            }
        }

        # –ü–æ–ª—É—á–∞–µ–º Study Instance UID –∏–∑ DICOM
        dicom_data = pydicom.dcmread(temp_file_path)
        study_instance_uid = dicom_data.StudyInstanceUID

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        existing_study = await db_manager.get_existing_study(study_instance_uid)

        if existing_study:
            logger.info(f"‚ö†Ô∏è Study with UID {study_instance_uid} already exists: {existing_study['study_id']}")
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π Study Instance UID –¥–ª—è –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏
            unique_study_uid = f"{study_instance_uid}.{study_id}"
            logger.info(f"‚úÖ Creating new study with modified UID: {unique_study_uid}")
        else:
            unique_study_uid = study_instance_uid

        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î
        try:
            await db_manager.create_study(
                study_id=study_id,
                study_instance_uid=unique_study_uid,
                user_token=user_data["token"],
                filename=file.filename,
                file_size=file.size
            )
        except Exception as db_error:
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –æ—à–∏–±–∫–∞ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è (—Ä–µ–¥–∫–∏–π —Å–ª—É—á–∞–π)
            if "duplicate key value violates unique constraint" in str(db_error):
                logger.warning(f"‚ö†Ô∏è Unexpected duplicate error, using timestamp suffix")
                timestamp_suffix = datetime.now().strftime("%Y%m%d%H%M%S")
                fallback_uid = f"{study_instance_uid}.{timestamp_suffix}"
                await db_manager.create_study(
                    study_id=study_id,
                    study_instance_uid=fallback_uid,
                    user_token=user_data["token"],
                    filename=file.filename,
                    file_size=file.size
                )
                logger.info(f"‚úÖ Created study with fallback UID: {fallback_uid}")
            else:
                raise db_error

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka
        await producer.send_and_wait(TOPIC_RAW, kafka_message)

        logger.info(f"‚úÖ File {file.filename} sent to processing, study_id: {study_id}")

        return ProcessingResponse(
            study_id=study_id,
            status="processing",
            message="–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É"
        )

    except HTTPException:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        raise

    except KafkaConnectionError as e:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ Kafka
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logger.error(f"‚ùå Kafka error: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail={"error_code": 503, "message": "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω", "severity": "critical"}
        )

    except Exception as e:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π –æ—à–∏–±–∫–µ
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logger.error(f"‚ùå Unexpected error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error_code": 500, "message": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞", "severity": "critical"}
        )


@app.get("/result/{study_id}")
async def get_result(
        study_id: str,
        user_data: dict = Depends(verify_token)
):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ study_id
    """
    # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –ë–î
    study = await db_manager.get_study(study_id)

    if not study:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Study not found"
        )

    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    response = {
        "study_id": study_id,
        "status": study['status'],
        "created_at": study['created_at'].isoformat() if study['created_at'] else None,
        "updated_at": study['updated_at'].isoformat() if study['updated_at'] else None,
        "filename": study['filename']
    }

    if study['status'] == 'completed':
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        if study['result_status']:
            response['results'] = {
                "status": study['result_status'],
                "atelectasis_probability": float(study['atelectasis_probability']) if study[
                    'atelectasis_probability'] else None,
                "processing_time": float(study['processing_time']) if study['processing_time'] else None,
                "conclusion": study['conclusion'],
                "location": study['location_description']
            }

            if study['bbox_xmin'] is not None:
                response['results']['bbox'] = [
                    study['bbox_xmin'],
                    study['bbox_ymin'],
                    study['bbox_xmax'],
                    study['bbox_ymax']
                ]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
        report_paths = await db_manager.get_report_paths(study_id)
        if report_paths:
            response['reports'] = report_paths

    elif study['status'] == 'error':
        response['error'] = study['error_message']

    return response


@app.get("/api/studies")
async def get_studies(
        limit: int = 50,
        offset: int = 0,
        user_data: dict = Depends(verify_token)
):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –≤—Å–µ—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –Ω—É–∂–Ω–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é)
    async with db_manager.acquire() as conn:
        rows = await conn.fetch("""
            SELECT s.study_id, s.filename, s.status, s.created_at, s.updated_at,
                   ar.atelectasis_probability, ar.conclusion
            FROM studies s
            LEFT JOIN analysis_results ar ON s.study_id = ar.study_id
            ORDER BY s.created_at DESC
            LIMIT $1 OFFSET $2
        """, limit, offset)

        studies = []
        for row in rows:
            study = dict(row)
            if study['created_at']:
                study['created_at'] = study['created_at'].isoformat()
            if study['updated_at']:
                study['updated_at'] = study['updated_at'].isoformat()
            studies.append(study)

        return {"studies": studies}


@app.get("/statistics")
async def get_statistics(
        user_data: dict = Depends(verify_token)
):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã
    """
    stats = await db_manager.get_statistics()
    return stats


@app.get("/download/reports/{study_id}")
async def download_reports(
        study_id: str,
        user_data: dict = Depends(verify_token)
):
    """
    –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤ –≤ –≤–∏–¥–µ ZIP –∞—Ä—Ö–∏–≤–∞
    """
    # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    report_paths = await db_manager.get_report_paths(study_id)

    logger.info(f"üìÅ Download request for study_id: {study_id}")
    logger.info(f"üìÅ Found report paths: {report_paths}")

    if not report_paths:
        logger.warning(f"‚ùå No report paths found for study_id: {study_id}")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Reports not found"
        )

    # –°–æ–∑–¥–∞–µ–º ZIP –∞—Ä—Ö–∏–≤ –≤ –ø–∞–º—è—Ç–∏
    zip_buffer = io.BytesIO()
    files_added = 0

    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
        for file_type, file_path in report_paths.items():
            logger.info(f"üìÑ Processing: {file_type} -> {file_path}")

            if file_type == 'dicom_series':
                # –≠—Ç–æ –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ - –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å—ë —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                if os.path.exists(file_path) and os.path.isdir(file_path):
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ dicom_series
                    for root, dirs, files in os.walk(file_path):
                        for file in files:
                            file_full_path = os.path.join(root, file)
                            # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç –±–∞–∑–æ–≤–æ–π –ø–∞–ø–∫–∏
                            relative_path = os.path.relpath(file_full_path, os.path.dirname(file_path))

                            logger.info(f"‚úÖ Adding to ZIP: {relative_path}")
                            zip_file.write(file_full_path, relative_path)
                            files_added += 1
                else:
                    logger.warning(f"‚ö†Ô∏è DICOM series directory not found: {file_path}")

            elif file_type in ['json_report', 'api_json']:
                # –¢–æ–ª—å–∫–æ JSON —Ñ–∞–π–ª—ã –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ—Ä–µ–Ω—å –∞—Ä—Ö–∏–≤–∞
                if os.path.exists(file_path):
                    try:
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –≤ –∫–æ—Ä–Ω–µ –∞—Ä—Ö–∏–≤–∞
                        if file_type == 'json_report':
                            archive_name = f"{study_id}_report.json"
                        elif file_type == 'api_json':
                            archive_name = f"{study_id}_api_report.json"

                        file_size = os.path.getsize(file_path)
                        logger.info(f"‚úÖ Adding JSON file {archive_name} (size: {file_size} bytes)")

                        zip_file.write(file_path, archive_name)
                        files_added += 1

                    except Exception as e:
                        logger.error(f"‚ùå Error adding file {file_path}: {e}")
                else:
                    logger.warning(f"‚ö†Ô∏è JSON file not found: {file_path}")

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º dicom_sr –∏ dicom_annotated, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ —É–∂–µ –≤–∫–ª—é—á–µ–Ω—ã –≤ dicom_series
            elif file_type in ['dicom_sr', 'dicom_annotated']:
                logger.info(f"‚è≠Ô∏è Skipping {file_type} (included in dicom_series folder)")
                continue

            else:
                # –î–ª—è –ª—é–±—ã—Ö –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤
                if os.path.exists(file_path):
                    try:
                        archive_name = f"{study_id}_{file_type}"
                        file_size = os.path.getsize(file_path)
                        logger.info(f"‚úÖ Adding other file {archive_name} (size: {file_size} bytes)")

                        zip_file.write(file_path, archive_name)
                        files_added += 1

                    except Exception as e:
                        logger.error(f"‚ùå Error adding file {file_path}: {e}")
                else:
                    logger.warning(f"‚ö†Ô∏è File not found: {file_path}")

    if files_added == 0:
        logger.warning(f"‚ùå No files were added to ZIP for study_id: {study_id}")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No report files found on disk"
        )

    zip_buffer.seek(0)
    archive_size = len(zip_buffer.getvalue())
    logger.info(f"üì¶ Created ZIP archive with {files_added} files, size: {archive_size} bytes")

    return StreamingResponse(
        io.BytesIO(zip_buffer.read()),
        media_type="application/zip",
        headers={"Content-Disposition": f"attachment; filename={study_id}_reports.zip"}
    )


@app.get("/download/dicom/{study_id}/{file_type}")
async def download_dicom_file(
        study_id: str,
        file_type: str,  # 'sr' –∏–ª–∏ 'annotated'
        user_data: dict = Depends(verify_token)
):
    """
    –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ DICOM —Ñ–∞–π–ª–∞
    """
    report_paths = await db_manager.get_report_paths(study_id)

    if file_type == 'sr' and 'dicom_sr' in report_paths:
        file_path = report_paths['dicom_sr']
        filename = f"{study_id}_structured_report.dcm"
    elif file_type == 'annotated' and 'dicom_annotated' in report_paths:
        file_path = report_paths['dicom_annotated']
        filename = f"{study_id}_annotated_image.dcm"
    else:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="DICOM file not found"
        )

    if not os.path.exists(file_path):
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="File not found on disk"
        )

    return FileResponse(
        file_path,
        media_type="application/dicom",
        filename=filename
    )


@app.post("/test/create_dicom")
async def create_test_dicom(
        png_file: UploadFile,
        add_patient_info: bool = True
):
    """
    –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DICOM –∏–∑ PNG (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
    """
    if not png_file.filename.endswith('.png'):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="–¢—Ä–µ–±—É–µ—Ç—Å—è PNG —Ñ–∞–π–ª"
        )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PNG –≤—Ä–µ–º–µ–Ω–Ω–æ
    temp_png = os.path.join(UPLOAD_DIR, f"temp_{png_file.filename}")
    temp_dcm = os.path.join(UPLOAD_DIR, f"test_{png_file.filename.replace('.png', '.dcm')}")

    try:
        with open(temp_png, "wb") as buffer:
            content = await png_file.read()
            buffer.write(content)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DICOM
        create_dicom_from_png(temp_png, temp_dcm, add_patient_info)

        return {
            "status": "success",
            "dicom_path": temp_dcm,
            "message": "DICOM —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω"
        }

    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π PNG
        if os.path.exists(temp_png):
            os.remove(temp_png)


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )