import os
import asyncio
import logging
import json
import tempfile
import shutil
from pathlib import Path
from datetime import datetime
from typing import Optional
import uuid

import uvicorn
from fastapi import FastAPI, UploadFile, HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import FileResponse
from pydantic import BaseModel
from aiokafka import AIOKafkaProducer
from aiokafka.errors import KafkaConnectionError
from dotenv import load_dotenv

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π —Å–µ—Ä–≤–∏—Å–∞
from validators import DicomValidator
from dicom_utils import create_dicom_from_png

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
current_dir = Path(__file__).resolve().parent
parent_dir = current_dir.parent.parent.parent
env_path = parent_dir / '.env'
load_dotenv(dotenv_path=env_path)

KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP", "kafka:9092")
TOPIC_RAW = os.getenv("TOPIC_RAW", "raw-images")
TOPIC_RESULTS = os.getenv("TOPIC_PROC", "inference-results")
UPLOAD_DIR = "./uploads"
MAX_FILE_SIZE = int(os.getenv("MAX_FILE_SIZE", "104857600"))  # 100MB

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–æ–∫
os.makedirs(UPLOAD_DIR, exist_ok=True)

app = FastAPI(title="Atelectasis Detection API", version="1.0.0")
security = HTTPBearer()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π producer –¥–ª—è Kafka
producer: Optional[AIOKafkaProducer] = None


# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ProcessingRequest(BaseModel):
    study_id: str
    file_path: str
    timestamp: str
    metadata: dict


class ProcessingResponse(BaseModel):
    study_id: str
    status: str
    message: str


class HealthResponse(BaseModel):
    status: str
    kafka_connected: bool
    timestamp: str


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Kafka
async def wait_for_kafka_ready(bootstrap_servers, max_retries=15, delay=5):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka"""
    for attempt in range(max_retries):
        try:
            test_producer = AIOKafkaProducer(
                bootstrap_servers=bootstrap_servers,
                request_timeout_ms=5000,
                connections_max_idle_ms=10000
            )
            await test_producer.start()
            logger.info("‚úÖ Kafka is ready!")
            await test_producer.stop()
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kafka not ready (attempt {attempt + 1}/{max_retries}): {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Kafka not ready after maximum retries")


# –°–æ–±—ã—Ç–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    global producer

    logger.info("üöÄ Starting API Gateway Service...")

    # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka
    try:
        await wait_for_kafka_ready(KAFKA_BOOTSTRAP)

        # –°–æ–∑–¥–∞–µ–º producer
        producer = AIOKafkaProducer(
            bootstrap_servers=KAFKA_BOOTSTRAP,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',
            retry_backoff_ms=2000,
            request_timeout_ms=30000
        )
        await producer.start()
        logger.info("‚úÖ Kafka producer started successfully")

    except Exception as e:
        logger.error(f"‚ùå Failed to initialize Kafka: {e}")
        raise


@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ"""
    global producer
    if producer:
        await producer.stop()
        logger.info("‚úÖ Kafka producer stopped")


# –§—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ JWT —Ç–æ–∫–µ–Ω–∞ (–∑–∞–≥–ª—É—à–∫–∞)
async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ JWT —Ç–æ–∫–µ–Ω–∞"""
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É JWT
    token = credentials.credentials
    if not token:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials"
        )
    return token


# Endpoints
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞"""
    kafka_connected = producer is not None and not producer._closed

    return HealthResponse(
        status="healthy" if kafka_connected else "degraded",
        kafka_connected=kafka_connected,
        timestamp=datetime.now().isoformat()
    )


@app.post("/analyze", response_model=ProcessingResponse)
async def analyze_dicom(
        file: UploadFile,
        credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑ DICOM —Ñ–∞–π–ª–∞
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    if file.size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
            detail={"error_code": 413, "message": "–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π", "severity": "critical"}
        )

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    study_id = str(uuid.uuid4())
    temp_file_path = None

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤—Ä–µ–º–µ–Ω–Ω–æ
        temp_file_path = os.path.join(UPLOAD_DIR, f"{study_id}_{file.filename}")

        with open(temp_file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)

        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º DICOM
        is_valid, error = DicomValidator.validate_dicom_file(temp_file_path)
        if not is_valid:
            raise HTTPException(
                status_code=error["error_code"],
                detail=error
            )

        # –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è Kafka
        kafka_message = {
            "study_id": study_id,
            "file_path": temp_file_path,
            "filename": file.filename,
            "timestamp": datetime.now().isoformat(),
            "user_token": credentials.credentials,
            "metadata": {
                "content_type": file.content_type,
                "file_size": file.size
            }
        }

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Kafka
        await producer.send_and_wait(TOPIC_RAW, kafka_message)

        logger.info(f"‚úÖ File {file.filename} sent to processing, study_id: {study_id}")

        return ProcessingResponse(
            study_id=study_id,
            status="processing",
            message="–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É"
        )

    except HTTPException:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        raise

    except KafkaConnectionError as e:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ Kafka
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logger.error(f"‚ùå Kafka error: {e}")
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail={"error_code": 503, "message": "–°–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω", "severity": "critical"}
        )

    except Exception as e:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π –æ—à–∏–±–∫–µ
        if temp_file_path and os.path.exists(temp_file_path):
            os.remove(temp_file_path)
        logger.error(f"‚ùå Unexpected error: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error_code": 500, "message": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞", "severity": "critical"}
        )


@app.get("/result/{study_id}")
async def get_result(
        study_id: str,
        credentials: HTTPAuthorizationCredentials = Depends(security)
):
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ study_id
    """
    # TODO: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ storage service
    # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É

    return {
        "study_id": study_id,
        "status": "processing",
        "message": "–†–µ–∑—É–ª—å—Ç–∞—Ç –µ—â–µ –Ω–µ –≥–æ—Ç–æ–≤"
    }


@app.post("/test/create_dicom")
async def create_test_dicom(
        png_file: UploadFile,
        add_patient_info: bool = True
):
    """
    –¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è DICOM –∏–∑ PNG (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
    """
    if not png_file.filename.endswith('.png'):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="–¢—Ä–µ–±—É–µ—Ç—Å—è PNG —Ñ–∞–π–ª"
        )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PNG –≤—Ä–µ–º–µ–Ω–Ω–æ
    temp_png = os.path.join(UPLOAD_DIR, f"temp_{png_file.filename}")
    temp_dcm = os.path.join(UPLOAD_DIR, f"test_{png_file.filename.replace('.png', '.dcm')}")

    try:
        with open(temp_png, "wb") as buffer:
            content = await png_file.read()
            buffer.write(content)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ DICOM
        create_dicom_from_png(temp_png, temp_dcm, add_patient_info)

        return {
            "status": "success",
            "dicom_path": temp_dcm,
            "message": "DICOM —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω"
        }

    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π PNG
        if os.path.exists(temp_png):
            os.remove(temp_png)


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )