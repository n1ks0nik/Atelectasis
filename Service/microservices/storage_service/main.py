#TODO: –ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–µ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—É—é –±–¥

import os
import asyncio
import logging
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

from dotenv import load_dotenv
from aiokafka import AIOKafkaConsumer
from aiokafka.errors import KafkaConnectionError, GroupCoordinatorNotAvailableError, KafkaError
from aiokafka.admin import AIOKafkaAdminClient, NewTopic
import pydicom

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π —Å–µ—Ä–≤–∏—Å–∞
from report_generator import DicomSRGenerator, generate_dicom_sr_from_json, generate_json_api_report

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
current_dir = Path(__file__).resolve().parent
parent_dir = current_dir.parent.parent
env_path = parent_dir / '.env'
load_dotenv(dotenv_path=env_path)

KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP", "kafka:9092")
TOPIC_RAW = os.getenv("TOPIC_RAW", "raw-images")
TOPIC_PROC = os.getenv("TOPIC_PROC", "inference-results")
GROUP_ID = "storage-service-group"
STORAGE_DIR = os.getenv("STORAGE_DIR", "./storage")
REPORTS_DIR = os.getenv("REPORTS_DIR", "./reports")

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs(STORAGE_DIR, exist_ok=True)
os.makedirs(os.path.join(REPORTS_DIR, "json"), exist_ok=True)
os.makedirs(os.path.join(REPORTS_DIR, "dicom_sr"), exist_ok=True)
os.makedirs(os.path.join(REPORTS_DIR, "json_api"), exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤
report_generator: DicomSRGenerator = None

# –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ - –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö)
results_storage: Dict[str, Any] = {}


async def wait_for_kafka_ready(bootstrap_servers, max_retries=15, delay=5):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka"""
    from aiokafka import AIOKafkaProducer

    for attempt in range(max_retries):
        try:
            producer = AIOKafkaProducer(
                bootstrap_servers=bootstrap_servers,
                request_timeout_ms=5000,
                connections_max_idle_ms=10000
            )
            await producer.start()
            logger.info(f"‚úÖ Kafka is ready! Connection successful")
            await producer.stop()
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kafka not ready (attempt {attempt + 1}/{max_retries}): {type(e).__name__}: {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Kafka not ready after maximum retries")


async def start_with_retries(component, max_retries=10, delay=3):
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    for attempt in range(max_retries):
        try:
            await component.start()
            logger.info(f"‚úÖ Component started successfully")
            return
        except (KafkaConnectionError, GroupCoordinatorNotAvailableError, KafkaError) as e:
            logger.warning(f"‚ö†Ô∏è Failed to start component (attempt {attempt + 1}/{max_retries}): {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Cannot start component after retries")


async def ensure_topics(bootstrap, topics):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–ø–∏–∫–æ–≤ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
    admin = AIOKafkaAdminClient(bootstrap_servers=bootstrap)
    await start_with_retries(admin, max_retries=5)

    try:
        existing = await admin.list_topics()
        to_create = [
            NewTopic(name=topic, num_partitions=1, replication_factor=1)
            for topic in topics if topic not in existing
        ]

        if to_create:
            await admin.create_topics(to_create)
            logger.info(f"‚úÖ Topics created: {[t.name for t in to_create]}")
        else:
            logger.info("‚úÖ All topics already exist")
    finally:
        await admin.close()


def initialize_storage_components():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    global report_generator

    logger.info("üíæ Initializing storage components...")
    report_generator = DicomSRGenerator()
    logger.info("‚úÖ Storage components initialized successfully")


async def process_and_store_result(result_data: Dict[str, Any]):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞
    """
    study_id = result_data.get("study_id")
    status = result_data.get("status")
    original_dicom_path = result_data.get("original_dicom_path")

    logger.info(f"üíæ Processing result for study_id: {study_id}, status: {status}")

    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        results_storage[study_id] = result_data

        if status == "completed" and result_data.get("results"):
            results = result_data["results"]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π JSON –æ—Ç—á–µ—Ç
            json_report_path = os.path.join(REPORTS_DIR, "json", f"{study_id}_report.json")
            with open(json_report_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            logger.info(f"‚úÖ JSON report saved: {json_report_path}")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º DICOM SR
            if original_dicom_path and os.path.exists(original_dicom_path):
                try:
                    dicom_files = report_generator.generate_complete_report(
                        json_report_path,
                        original_dicom_path,
                        os.path.join(REPORTS_DIR, "dicom_sr"),
                        study_id
                    )
                    
                    if dicom_files:
                        logger.info(f"‚úÖ Complete DICOM report generated: {len(dicom_files)} files")
                        result_data["dicom_files"] = dicom_files
                        result_data["dicom_series_path"] = os.path.join(REPORTS_DIR, "dicom_sr", study_id)
                except Exception as e:
                    logger.error(f"‚ùå Failed to generate DICOM report: {e}")
                    import traceback
                    traceback.print_exc()
                    
            else:
                logger.warning(f"‚ö†Ô∏è Original DICOM not found at: {original_dicom_path}")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON –æ—Ç—á–µ—Ç –¥–ª—è API
            try:
                api_json_path = os.path.join(REPORTS_DIR, "json_api", f"{study_id}_api.json")
                success = report_generator.generate_json_report(json_report_path, api_json_path)
                if success:
                    logger.info(f"‚úÖ API JSON report generated: {api_json_path}")
                    result_data["api_json_path"] = api_json_path
            except Exception as e:
                logger.error(f"‚ùå Failed to generate API JSON: {e}")

            # –û–±–Ω–æ–≤–ª—è–µ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –ø—É—Ç—è–º–∏ –∫ –æ—Ç—á–µ—Ç–∞–º
            results_storage[study_id] = result_data

            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            if results.get("status") == "atelectasis_only":
                logger.info(f"üî¥ Atelectasis detected! Probability: {results.get('atelectasis_probability', 0):.2%}")
            elif results.get("status") == "normal":
                logger.info(
                    f"üü¢ Normal result. Atelectasis probability: {results.get('atelectasis_probability', 0):.2%}")
            elif results.get("status") == "other_pathologies":
                logger.info(
                    f"üü° Other pathologies detected. Atelectasis probability: {results.get('atelectasis_probability', 0):.2%}"
                )
        elif status == "error":
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
            error_info = {
                "study_id": study_id,
                "status": "error",
                "error": result_data.get("error"),
                "timestamp": result_data.get("timestamp_processed"),
                "processing_time": result_data.get("processing_time")
            }

            error_path = os.path.join(REPORTS_DIR, "json", f"{study_id}_error.json")
            with open(error_path, 'w', encoding='utf-8') as f:
                json.dump(error_info, f, ensure_ascii=False, indent=4)

            logger.error(f"‚ùå Error result saved: {error_path}")

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
        result_data["stored_at"] = datetime.now().isoformat()
        results_storage[study_id] = result_data

        logger.info(f"‚úÖ Result successfully stored for study_id: {study_id}")

    except Exception as e:
        logger.error(f"‚ùå Failed to store result for study_id {study_id}: {e}")
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—à–∏–±–∫–µ
        results_storage[study_id] = {
            "study_id": study_id,
            "status": "storage_error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

    if original_dicom_path and os.path.exists(original_dicom_path):
        try:
            os.remove(original_dicom_path)
            logger.info(f"üóëÔ∏è Temporary DICOM file removed: {original_dicom_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to remove temporary file: {e}")


def get_result_by_study_id(study_id: str) -> Optional[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ study_id
    –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
    """
    return results_storage.get(study_id)


async def cleanup_old_results():
    """
    –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    while True:
        try:
            await asyncio.sleep(3600)  # –ö–∞–∂–¥—ã–π —á–∞—Å

            current_time = datetime.now()
            to_remove = []

            for study_id, result in results_storage.items():
                # –£–¥–∞–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞—Ä—à–µ 24 —á–∞—Å–æ–≤
                if "stored_at" in result:
                    stored_time = datetime.fromisoformat(result["stored_at"])
                    if (current_time - stored_time).total_seconds() > 86400:  # 24 —á–∞—Å–∞
                        to_remove.append(study_id)

            for study_id in to_remove:
                del results_storage[study_id]
                logger.info(f"üóëÔ∏è Removed old result: {study_id}")

            if to_remove:
                logger.info(f"‚úÖ Cleaned up {len(to_remove)} old results")

        except Exception as e:
            logger.error(f"‚ùå Error in cleanup task: {e}")


async def store_loop():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    logger.info("üöÄ Starting Storage Service...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    initialize_storage_components()

    # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka
    await wait_for_kafka_ready(KAFKA_BOOTSTRAP)

    # –°–æ–∑–¥–∞–µ–º —Ç–æ–ø–∏–∫–∏
    await ensure_topics(KAFKA_BOOTSTRAP, [TOPIC_RAW, TOPIC_PROC])

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
    logger.info("‚è≥ Waiting for system stabilization...")
    await asyncio.sleep(10)

    # –°–æ–∑–¥–∞–µ–º consumer
    consumer = AIOKafkaConsumer(
        TOPIC_PROC,
        bootstrap_servers=KAFKA_BOOTSTRAP,
        group_id=GROUP_ID,
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        auto_commit_interval_ms=1000,
        consumer_timeout_ms=1000,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
    await start_with_retries(consumer)

    logger.info("‚úÖ Storage Service ready, waiting for results...")

    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É –æ—á–∏—Å—Ç–∫–∏ –≤ —Ñ–æ–Ω–µ
    cleanup_task = asyncio.create_task(cleanup_old_results())

    try:
        while True:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                msg_batch = await consumer.getmany(timeout_ms=1000, max_records=10)

                if not msg_batch:
                    continue

                for topic_partition, messages in msg_batch.items():
                    for msg in messages:
                        logger.info(f"üì® Received result from {topic_partition}, offset: {msg.offset}")

                        try:
                            result_data = msg.value

                            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                            if not isinstance(result_data, dict) or "study_id" not in result_data:
                                logger.error(f"‚ùå Invalid result format: {result_data}")
                                continue

                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                            await process_and_store_result(result_data)

                            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                            total_results = len(results_storage)
                            completed = sum(1 for r in results_storage.values() if r.get("status") == "completed")
                            errors = sum(1 for r in results_storage.values() if r.get("status") == "error")

                            logger.info(
                                f"üìä Storage stats - Total: {total_results}, Completed: {completed}, Errors: {errors}")

                        except Exception as e:
                            logger.error(f"‚ùå Error processing result: {e}")
                            import traceback
                            traceback.print_exc()

            except GroupCoordinatorNotAvailableError as e:
                logger.warning(f"‚ö†Ô∏è GroupCoordinator not available: {e}, retrying...")
                await asyncio.sleep(5)
                continue
            except Exception as e:
                logger.error(f"‚ùå Error in storage loop: {e}")
                await asyncio.sleep(5)
                continue

    except KeyboardInterrupt:
        logger.info("üõë Shutting down Storage Service...")
    finally:
        cleanup_task.cancel()
        await consumer.stop()
        logger.info("‚úÖ Storage Service stopped")


# API endpoints –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å API Gateway)
async def get_study_result(study_id: str) -> Optional[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ ID
    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ gRPC –∏–ª–∏ REST API
    """
    result = get_result_by_study_id(study_id)

    if not result:
        return None

    # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è API
    response = {
        "study_id": study_id,
        "status": result.get("status"),
        "processing_time": result.get("processing_time"),
        "timestamp_processed": result.get("timestamp_processed")
    }

    if result.get("status") == "completed":
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        if result.get("results"):
            response["results"] = result["results"]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏ –∫ –æ—Ç—á–µ—Ç–∞–º
        if result.get("api_json_path") and os.path.exists(result["api_json_path"]):
            with open(result["api_json_path"], 'r', encoding='utf-8') as f:
                response["detailed_report"] = json.load(f)

        if result.get("dicom_sr_path"):
            response["dicom_sr_available"] = True
            response["dicom_sr_path"] = result["dicom_sr_path"]

    elif result.get("status") == "error":
        response["error"] = result.get("error")

    return response


async def get_study_statistics() -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º
    """
    total = len(results_storage)
    completed = sum(1 for r in results_storage.values() if r.get("status") == "completed")
    errors = sum(1 for r in results_storage.values() if r.get("status") == "error")
    processing = sum(1 for r in results_storage.values() if r.get("status") == "processing")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–∞—Ç–æ–ª–æ–≥–∏—è–º
    atelectasis_count = 0
    normal_count = 0
    other_pathologies_count = 0

    for result in results_storage.values():
        if result.get("status") == "completed" and result.get("results"):
            results = result["results"]
            if results.get("status") == "atelectasis_only":
                atelectasis_count += 1
            elif results.get("status") == "normal":
                normal_count += 1
            elif results.get("status") == "other_pathologies":
                other_pathologies_count += 1

    # –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∞—Ç–µ–ª–µ–∫—Ç–∞–∑–∞
    atelectasis_probs = []
    for result in results_storage.values():
        if result.get("status") == "completed" and result.get("results"):
            prob = result["results"].get("atelectasis_probability")
            if prob is not None:
                atelectasis_probs.append(prob)

    avg_atelectasis_prob = sum(atelectasis_probs) / len(atelectasis_probs) if atelectasis_probs else 0

    # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    processing_times = []
    for result in results_storage.values():
        if result.get("processing_time") is not None:
            processing_times.append(result["processing_time"])

    avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0

    return {
        "total_studies": total,
        "completed": completed,
        "errors": errors,
        "processing": processing,
        "pathology_statistics": {
            "atelectasis": atelectasis_count,
            "normal": normal_count,
            "other_pathologies": other_pathologies_count
        },
        "average_atelectasis_probability": avg_atelectasis_prob,
        "average_processing_time": avg_processing_time,
        "last_update": datetime.now().isoformat()
    }


# –¢–û–ß–ö–ê –í–•–û–î–ê - –≠–¢–û –ë–´–õ–û –ü–†–û–ü–£–©–ï–ù–û!
if __name__ == "__main__":
    try:
        asyncio.run(store_loop())
    except KeyboardInterrupt:
        logger.info("üëã Storage Service terminated by user")