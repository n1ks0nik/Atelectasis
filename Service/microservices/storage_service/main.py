import os
import asyncio
import logging
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

from dotenv import load_dotenv
from aiokafka import AIOKafkaConsumer
from aiokafka.errors import KafkaConnectionError, GroupCoordinatorNotAvailableError, KafkaError
from aiokafka.admin import AIOKafkaAdminClient, NewTopic
from shared.database import db_manager

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π —Å–µ—Ä–≤–∏—Å–∞
from report_generator import DicomSRGenerator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

load_dotenv()

KAFKA_BOOTSTRAP = os.getenv("KAFKA_BOOTSTRAP", "kafka:9092")
TOPIC_RAW = os.getenv("TOPIC_RAW", "raw-images")
TOPIC_PROC = os.getenv("TOPIC_PROC", "inference-results")
GROUP_ID = "storage-service-group"
STORAGE_DIR = os.getenv("STORAGE_DIR", "./storage")
REPORTS_DIR = os.getenv("REPORTS_DIR", "./reports")

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs(STORAGE_DIR, exist_ok=True)
os.makedirs(os.path.join(REPORTS_DIR, "json"), exist_ok=True)
os.makedirs(os.path.join(REPORTS_DIR, "dicom_sr"), exist_ok=True)
os.makedirs(os.path.join(REPORTS_DIR, "json_api"), exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤
report_generator: DicomSRGenerator = None


async def wait_for_kafka_ready(bootstrap_servers, max_retries=15, delay=5):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka"""
    from aiokafka import AIOKafkaProducer

    for attempt in range(max_retries):
        try:
            producer = AIOKafkaProducer(
                bootstrap_servers=bootstrap_servers,
                request_timeout_ms=5000,
                connections_max_idle_ms=10000
            )
            await producer.start()
            logger.info(f"‚úÖ Kafka is ready! Connection successful")
            await producer.stop()
            return True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kafka not ready (attempt {attempt + 1}/{max_retries}): {type(e).__name__}: {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Kafka not ready after maximum retries")


async def start_with_retries(component, max_retries=10, delay=3):
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    for attempt in range(max_retries):
        try:
            await component.start()
            logger.info(f"‚úÖ Component started successfully")
            return
        except (KafkaConnectionError, GroupCoordinatorNotAvailableError, KafkaError) as e:
            logger.warning(f"‚ö†Ô∏è Failed to start component (attempt {attempt + 1}/{max_retries}): {e}")
            await asyncio.sleep(delay)

    raise RuntimeError("‚ùå Cannot start component after retries")


async def ensure_topics(bootstrap, topics):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–ø–∏–∫–æ–≤ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç"""
    admin = AIOKafkaAdminClient(bootstrap_servers=bootstrap)
    await start_with_retries(admin, max_retries=5)

    try:
        existing = await admin.list_topics()
        to_create = [
            NewTopic(name=topic, num_partitions=1, replication_factor=1)
            for topic in topics if topic not in existing
        ]

        if to_create:
            await admin.create_topics(to_create)
            logger.info(f"‚úÖ Topics created: {[t.name for t in to_create]}")
        else:
            logger.info("‚úÖ All topics already exist")
    finally:
        await admin.close()


async def connect_to_database():
    """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    max_retries = 10
    for attempt in range(max_retries):
        try:
            await db_manager.connect()
            logger.info("‚úÖ Connected to PostgreSQL")
            return
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to connect to DB (attempt {attempt + 1}/{max_retries}): {e}")
            await asyncio.sleep(5)

    raise RuntimeError("‚ùå Cannot connect to database")


def initialize_storage_components():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    global report_generator

    logger.info("üíæ Initializing storage components...")
    report_generator = DicomSRGenerator()

    asyncio.create_task(connect_to_database())

    logger.info("‚úÖ Storage components initialized successfully")


async def process_and_store_result(result_data: Dict[str, Any]):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞
    """
    study_id = result_data.get("study_id")
    status = result_data.get("status")
    original_dicom_path = result_data.get("original_dicom_path")

    logger.info(f"üíæ Processing result for study_id: {study_id}, status: {status}")

    try:
        await db_manager.save_analysis_result(study_id, result_data)

        if status == "completed" and result_data.get("results"):
            results = result_data["results"]
            report_paths = {}

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π JSON –æ—Ç—á–µ—Ç
            json_report_path = os.path.join(REPORTS_DIR, "json", f"{study_id}_report.json")
            with open(json_report_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=4)
            logger.info(f"‚úÖ JSON report saved: {json_report_path}")
            report_paths['json_report'] = json_report_path

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º DICOM SR
            if original_dicom_path and os.path.exists(original_dicom_path):
                try:
                    dicom_files = report_generator.generate_complete_report(
                        json_report_path,
                        original_dicom_path,
                        os.path.join(REPORTS_DIR, "dicom_sr"),
                        study_id
                    )

                    if dicom_files:
                        logger.info(f"‚úÖ Complete DICOM report generated: {len(dicom_files)} files")
                        report_paths['dicom_series'] = os.path.join(REPORTS_DIR, "dicom_sr", study_id)

                        for file_path in dicom_files:
                            if 'annotated' in file_path:
                                report_paths['dicom_annotated'] = file_path
                            elif 'sr' in file_path:
                                report_paths['dicom_sr'] = file_path

                except Exception as e:
                    logger.error(f"‚ùå Failed to generate DICOM report: {e}")
                    import traceback
                    traceback.print_exc()

            else:
                logger.warning(f"‚ö†Ô∏è Original DICOM not found at: {original_dicom_path}")

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON –æ—Ç—á–µ—Ç –¥–ª—è API
            try:
                api_json_path = os.path.join(REPORTS_DIR, "json_api", f"{study_id}_api.json")
                success = report_generator.generate_json_report(json_report_path, api_json_path)
                if success:
                    logger.info(f"‚úÖ API JSON report generated: {api_json_path}")
                    report_paths['api_json'] = api_json_path
            except Exception as e:
                logger.error(f"‚ùå Failed to generate API JSON: {e}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –≤ –ë–î
            await db_manager.save_report_paths(study_id, report_paths)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ completed
            await db_manager.update_study_status(study_id, 'completed')


        elif status == "error":
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ error
            await db_manager.update_study_status(study_id, 'error')

        logger.info(f"‚úÖ Result successfully stored in database for study_id: {study_id}")

    except Exception as e:
        logger.error(f"‚ùå Failed to store result for study_id {study_id}: {e}")
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ storage_error
        try:
            await db_manager.update_study_status(study_id, 'storage_error')
        except:
            pass

    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ—Å–ª–µ –≤—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if original_dicom_path and os.path.exists(original_dicom_path):
            try:
                os.remove(original_dicom_path)
                logger.info(f"üóëÔ∏è Temporary DICOM file removed: {original_dicom_path}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to remove temporary file: {e}")


async def store_loop():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    logger.info("üöÄ Starting Storage Service...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    initialize_storage_components()

    # –ñ–¥–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ Kafka
    await wait_for_kafka_ready(KAFKA_BOOTSTRAP)

    # –°–æ–∑–¥–∞–µ–º —Ç–æ–ø–∏–∫–∏
    await ensure_topics(KAFKA_BOOTSTRAP, [TOPIC_RAW, TOPIC_PROC])

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏
    logger.info("‚è≥ Waiting for system stabilization...")
    await asyncio.sleep(10)

    # –°–æ–∑–¥–∞–µ–º consumer
    consumer = AIOKafkaConsumer(
        TOPIC_PROC,
        bootstrap_servers=KAFKA_BOOTSTRAP,
        group_id=GROUP_ID,
        auto_offset_reset='earliest',
        enable_auto_commit=True,
        auto_commit_interval_ms=1000,
        consumer_timeout_ms=1000,
        value_deserializer=lambda m: json.loads(m.decode('utf-8'))
    )

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏
    await start_with_retries(consumer)

    logger.info("‚úÖ Storage Service ready, waiting for results...")

    try:
        while True:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                msg_batch = await consumer.getmany(timeout_ms=1000, max_records=10)

                if not msg_batch:
                    continue

                for topic_partition, messages in msg_batch.items():
                    for msg in messages:
                        logger.info(f"üì® Received result from {topic_partition}, offset: {msg.offset}")

                        try:
                            result_data = msg.value

                            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
                            if not isinstance(result_data, dict) or "study_id" not in result_data:
                                logger.error(f"‚ùå Invalid result format: {result_data}")
                                continue

                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                            await process_and_store_result(result_data)

                        except Exception as e:
                            logger.error(f"‚ùå Error processing result: {e}")
                            import traceback
                            traceback.print_exc()

            except GroupCoordinatorNotAvailableError as e:
                logger.warning(f"‚ö†Ô∏è GroupCoordinator not available: {e}, retrying...")
                await asyncio.sleep(5)
                continue
            except Exception as e:
                logger.error(f"‚ùå Error in storage loop: {e}")
                await asyncio.sleep(5)
                continue

    except KeyboardInterrupt:
        logger.info("üõë Shutting down Storage Service...")
    finally:
        await consumer.stop()
        await db_manager.disconnect()
        logger.info("‚úÖ Storage Service stopped")


if __name__ == "__main__":
    try:
        asyncio.run(store_loop())
    except KeyboardInterrupt:
        logger.info("üëã Storage Service terminated by user")